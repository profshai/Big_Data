{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python [conda root]",
      "language": "python",
      "name": "conda-root-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    },
    "colab": {
      "name": "tools_for_nlp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/profshai/pyspark-big-data/blob/main/tools_for_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhDDysR2XP0Q"
      },
      "source": [
        "# Tools for Natural Language Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78eZGVXjbgdI",
        "outputId": "761995ca-038a-48fc-c32b-5ae7a2601ecd"
      },
      "source": [
        "pip install pyspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/db/e18cfd78e408de957821ec5ca56de1250645b05f8523d169803d8df35a64/pyspark-3.1.2.tar.gz (212.4MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4MB 63kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 15.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=f0ca7a3e9034ebbabf28fc4013251491698120f529585ce9382167f166c22cb9\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/1b/2c/30f43be2627857ab80062bef1527c0128f7b4070b6b2d02139\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "oP6TRg4tXP0T"
      },
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "4onKAZy-XP0T"
      },
      "source": [
        "spark = SparkSession.builder.appName('nlp').getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDT4dvN-XP0U"
      },
      "source": [
        "## Tokenizer\n",
        "Tokenization is the process of taking text (such as a sentence) and breaking it into individual terms (usually words).  \n",
        "\n",
        "RegexTokenizer allows more advanced tokenization based on regular expression (regex) matching."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "dT2_8vTdXP0V"
      },
      "source": [
        "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import IntegerType"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "gtP6ZZ8iXP0V"
      },
      "source": [
        "sentenceDataFrame = spark.createDataFrame([\n",
        "    (0, \"Hi I heard about Spark\"),\n",
        "    (1, \"I wish Java could use case classes\"),\n",
        "    (2, \"Logistic,regression,models,are,neat\")\n",
        "], [\"id\", \"sentence\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMcz6EgqXP0W",
        "outputId": "f98ac49a-e1c5-4293-d771-9b6fa94f814d"
      },
      "source": [
        "sentenceDataFrame.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------------------+\n",
            "| id|            sentence|\n",
            "+---+--------------------+\n",
            "|  0|Hi I heard about ...|\n",
            "|  1|I wish Java could...|\n",
            "|  2|Logistic,regressi...|\n",
            "+---+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1OU6QY_XP0Y",
        "outputId": "a29e3086-7712-4a24-f974-03105f11d0f3"
      },
      "source": [
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
        "regexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")\n",
        "\n",
        "countTokens = udf(lambda words: len(words), IntegerType())\n",
        "\n",
        "tokenized = tokenizer.transform(sentenceDataFrame)\n",
        "tokenized.select(\"sentence\", \"words\")\\\n",
        "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)\n",
        "\n",
        "regexTokenized = regexTokenizer.transform(sentenceDataFrame)\n",
        "regexTokenized.select(\"sentence\", \"words\") \\\n",
        "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------------------------+------------------------------------------+------+\n",
            "|sentence                           |words                                     |tokens|\n",
            "+-----------------------------------+------------------------------------------+------+\n",
            "|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n",
            "|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n",
            "|Logistic,regression,models,are,neat|[logistic,regression,models,are,neat]     |1     |\n",
            "+-----------------------------------+------------------------------------------+------+\n",
            "\n",
            "+-----------------------------------+------------------------------------------+------+\n",
            "|sentence                           |words                                     |tokens|\n",
            "+-----------------------------------+------------------------------------------+------+\n",
            "|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n",
            "|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n",
            "|Logistic,regression,models,are,neat|[logistic, regression, models, are, neat] |5     |\n",
            "+-----------------------------------+------------------------------------------+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-VgHhJ5XP0a"
      },
      "source": [
        "\n",
        "### Remove Stopwords\n",
        "\n",
        "Stop words are words which should be excluded from the input, typically because the words appear frequently and don't carry as much meaning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E28Hyoj3XP0a",
        "outputId": "ec67aaaa-6d3d-4592-ff4a-3847dff1c873"
      },
      "source": [
        "from pyspark.ml.feature import StopWordsRemover\n",
        "\n",
        "sentenceData = spark.createDataFrame([\n",
        "    (0, [\"I\", \"saw\", \"the\", \"red\", \"balloon\"]),\n",
        "    (1, [\"Mary\", \"had\", \"a\", \"little\", \"lamb\"])\n",
        "], [\"id\", \"raw\"])\n",
        "\n",
        "remover = StopWordsRemover(inputCol=\"raw\", outputCol=\"filtered\")\n",
        "remover.transform(sentenceData).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+----------------------------+--------------------+\n",
            "|id |raw                         |filtered            |\n",
            "+---+----------------------------+--------------------+\n",
            "|0  |[I, saw, the, red, balloon] |[saw, red, balloon] |\n",
            "|1  |[Mary, had, a, little, lamb]|[Mary, little, lamb]|\n",
            "+---+----------------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qh5t25HlXP0b"
      },
      "source": [
        "### n-grams\n",
        "\n",
        "An n-gram is a sequence of nn tokens (typically words) for some integer nn. The NGram class can be used to transform input features into nn-grams.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmXjoTV2XP0b",
        "outputId": "1d72b32d-b18f-4067-c095-397f2d97eb0f"
      },
      "source": [
        "from pyspark.ml.feature import NGram\n",
        "\n",
        "wordDataFrame = spark.createDataFrame([\n",
        "    (0, [\"Hi\", \"I\", \"heard\", \"about\", \"Spark\"]),\n",
        "    (1, [\"I\", \"wish\", \"Java\", \"could\", \"use\", \"case\", \"classes\"]),\n",
        "    (2, [\"Logistic\", \"regression\", \"models\", \"are\", \"neat\"])\n",
        "], [\"id\", \"words\"])\n",
        "\n",
        "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n",
        "\n",
        "ngramDataFrame = ngram.transform(wordDataFrame)\n",
        "ngramDataFrame.select(\"ngrams\").show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------------------------------------------------------+\n",
            "|ngrams                                                            |\n",
            "+------------------------------------------------------------------+\n",
            "|[Hi I, I heard, heard about, about Spark]                         |\n",
            "|[I wish, wish Java, Java could, could use, use case, case classes]|\n",
            "|[Logistic regression, regression models, models are, are neat]    |\n",
            "+------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6k9isWyJXP0c"
      },
      "source": [
        "### Feature Extractors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajf4joU0XP0c"
      },
      "source": [
        "#### TF-IDF\n",
        "\n",
        "Term frequency-inverse document frequency (TF-IDF) is a feature vectorization method widely used in text mining to reflect the importance of a term \n",
        "to a document in the corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ec7kBN_XP0d",
        "outputId": "c6670308-b2d8-4518-b423-5bcb19be86a5"
      },
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
        "\n",
        "sentenceData = spark.createDataFrame([\n",
        "    (0.0, \"Hi I heard about Spark\"),\n",
        "    (0.0, \"I wish Java could use case classes\"),\n",
        "    (1.0, \"Logistic regression models are neat\")\n",
        "], [\"label\", \"sentence\"])\n",
        "\n",
        "sentenceData.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|label|            sentence|\n",
            "+-----+--------------------+\n",
            "|  0.0|Hi I heard about ...|\n",
            "|  0.0|I wish Java could...|\n",
            "|  1.0|Logistic regressi...|\n",
            "+-----+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3NZ232DXP0d",
        "outputId": "1c96532d-5e81-434b-de8c-282d5a8908ac"
      },
      "source": [
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
        "wordsData = tokenizer.transform(sentenceData)\n",
        "wordsData.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+-----------------------------------+------------------------------------------+\n",
            "|label|sentence                           |words                                     |\n",
            "+-----+-----------------------------------+------------------------------------------+\n",
            "|0.0  |Hi I heard about Spark             |[hi, i, heard, about, spark]              |\n",
            "|0.0  |I wish Java could use case classes |[i, wish, java, could, use, case, classes]|\n",
            "|1.0  |Logistic regression models are neat|[logistic, regression, models, are, neat] |\n",
            "+-----+-----------------------------------+------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPzdkLFbXP0d",
        "outputId": "812bce30-30bc-4d2a-b223-f58c4720560f"
      },
      "source": [
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
        "featurizedData = hashingTF.transform(wordsData)\n",
        "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
        "\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "idfModel = idf.fit(featurizedData)\n",
        "rescaledData = idfModel.transform(featurizedData)\n",
        "\n",
        "rescaledData.select(\"label\", \"features\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|(20,[6,8,13,16],[...|\n",
            "|  0.0|(20,[0,2,7,13,15,...|\n",
            "|  1.0|(20,[3,4,6,11,19]...|\n",
            "+-----+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__yyVQBjXP0d"
      },
      "source": [
        "### CountVectorizer\n",
        "CountVectorizer and CountVectorizerModel aim to help convert a collection of text documents to vectors of token counts. \n",
        "\n",
        "When an a-priori dictionary is not available, CountVectorizer can be used as an Estimator to extract the vocabulary, and generates a CountVectorizerModel. The model produces sparse representations for the documents over the vocabulary, which can then be passed to other algorithms like LDA."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63YogKFDXP0e",
        "outputId": "0d1efa98-8a54-4e58-9b7c-2ebf193df156"
      },
      "source": [
        "from pyspark.ml.feature import CountVectorizer\n",
        "\n",
        "# Input data: Each row is a bag of words with a ID.\n",
        "df = spark.createDataFrame([\n",
        "    (0, \"a b c\".split(\" \")),\n",
        "    (1, \"a b b c a\".split(\" \"))\n",
        "], [\"id\", \"words\"])\n",
        "\n",
        "# fit a CountVectorizerModel from the corpus.\n",
        "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=3, minDF=2.0)\n",
        "\n",
        "model = cv.fit(df)\n",
        "\n",
        "result = model.transform(df)\n",
        "result.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---------------+-------------------------+\n",
            "|id |words          |features                 |\n",
            "+---+---------------+-------------------------+\n",
            "|0  |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n",
            "|1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n",
            "+---+---------------+-------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH0Bk-rDXT_j"
      },
      "source": [
        "End of Notebook!"
      ]
    }
  ]
}